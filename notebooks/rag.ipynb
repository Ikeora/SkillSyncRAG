{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Keys retrieval\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# Data Collection\n",
    "import requests\n",
    "import pprint\n",
    "\n",
    "# Storing Data\n",
    "import uuid \n",
    "import requests\n",
    "import pinecone\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ChromaDB\n",
    "import os\n",
    "import chromadb\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Data Wrangling & Cleaning\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import hashlib\n",
    "import datetime\n",
    "import re\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import re\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the RAG model; we'll focus on using templates from langchain-crash-course-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd() # NOTE: use this for .ipynb files\n",
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_with_metadata\")\n",
    "\n",
    "# Initialize embedding model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# embeddings = OpenAIEmbeddings(\n",
    "#     model=\"text-embedding-3-small\"\n",
    "# )\n",
    "\n",
    "# Init ChromaDB collection once\n",
    "collection_name = \"jobs_collection\"\n",
    "db = Chroma(\n",
    "    collection_name=collection_name,\n",
    "    persist_directory=persistent_directory,\n",
    "    embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing vector store with the embedding function\n",
    "db = Chroma(\n",
    "    persist_directory=persistent_directory,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"jobs_collection\"  # Match the name used in push\n",
    ")\n",
    "\n",
    "# Define the user's question\n",
    "query = \"What are the skills I need to get a data scientist job?\"\n",
    "\n",
    "# Retrieve relevant documents based on the query\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 20, \"score_threshold\": 0.25},  # Lower threshold for testing\n",
    ")\n",
    "# `k`: Top # of document hits \n",
    "# `score_threshold`: Similarity score from text document to query\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "# Display the relevant results with metadata\n",
    "print(\"\\n--- Relevant Documents ---\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "    if doc.metadata:\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting with ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relevant Documents ---\n",
      "Document 1:\n",
      "MINIMUM QUALIFICATIONS:\n",
      "• Bachelor’s Degree required OR equivalent combination of training, education, and relevant experience may be considered in lieu of a degree.\n",
      "• Advanced analytical knowledge of data\n",
      "• Statistical analysis\n",
      "• Conducting big data analysis\n",
      "• Data conditioning\n",
      "• Programming advanced computing\n",
      "• Developing machine learning algorithms (classification, regression, clustering, model validation, etc.)\n",
      "• Developing software and data models\n",
      "• Executing predictive analytics\n",
      "• Proficient with one or more programming languages (Python, R, SQL, Alteryx, SAS, etc.) and hands-on working experience with database systems, business intelligence, and visual reporting tools (BOBJ, Tableau, Power BI, etc.\n",
      "\n",
      "WORK ENVIRONMENT:\n",
      "• Work is performed 4 days in the office, with 1 day remote.\n",
      "• May require traveling up to 10% of the time.\n",
      "• Additional duties as required.\n",
      "\n",
      "COMMUNICATIONS AND INTERPERSONAL SKILLS:\n",
      "\n",
      "Must have excellent oral and written communication skills.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 2:\n",
      "PREFERRED QUALIFICATIONS:\n",
      "• Master’s or PhD in in Statistics, Computer Science, Data Analytics, Operations Research, Mathematics, Physics, Economics, Finance, or other quantitative disciplines.\n",
      "• Strong problem-solving skills with an emphasis on product development.\n",
      "• Sound knowledge of statistical methods and machine learning algorithms.\n",
      "• High degree of proficiency in Python and SQL.\n",
      "• 1+ years of experience in collecting, integrating, processing, and analyzing data.\n",
      "• 1+ years of experience in using open source/commercial data visualization libraries/tools.\n",
      "• 1+ years of experience with cloud services (AWS, Azure).\n",
      "• 1+ years of experience in version control workflows and technologies (Git, GitHub/Gitlab)\n",
      "• Ability to analyze, summarize and cogently present quantitative and qualitative information.\n",
      "• Passion for implementing industry standards / best coding practices.\n",
      "• Excellent written and verbal communication skills for coordinating across teams.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 3:\n",
      "M.Sc. in Data Science or related with 1+ years' experience\n",
      "• or- B.Sc. in Data Science or related with 3+ years' experience Technical Skills Computer Science Requirements Proficient in at least one major programming language (Python, Scala, Java, C#, C/C++, Kotlin). Hands-on experience with Git (handling branching and merging) Solid knowledge of data structures, algorithm design, problem solving, and complexity analysis. Understanding of database theory and experience in at least one relational DBMS like SQL Server, PostgreSQL, and MySQL. Expertise in data manipulation\n",
      "\n",
      "Desired Skills\n",
      "\n",
      "Big Data analytics with spark Real-time analytics Apache Spark machine learning library Familiarity with frameworks and tools such as Databricks, RabbitMQ, Kafka, Docker or Kubernetes. Familiarity with Microsoft Azure Analytics Platform. Familiarity with the mining Industry, focused on load & haul operations. Experience with agile methodologies.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 4:\n",
      "Minimum Qualifications\n",
      "• B.S. in a quantitative field (e.g., Statistics, Economics, Computer Science, Mathematics, Engineering).\n",
      "• At least 5+ years of experience as a Data Analytics Engineer with excellent analytical and problem-solving skills.\n",
      "• Advanced programming skills in data manipulation & processing (SQL, Python and Scala).\n",
      "• Experience in the big data ecosystem (e.g. Hadoop, Spark, Iceberg, Trino, Airflow).\n",
      "• Proven expertise in data wrangling and developing data visualizations & reporting with toolings such as Tableau, Superset etc.\n",
      "• Strong understanding of data analytics, structured and unstructured data analysis, predictive modeling techniques to implement Fraud detection, identify patterns in data and data visualization as well as a good command of emerging methodologies like artificial intelligence.\n",
      "• Outstanding verbal and written communication skills, along with strong collaborative abilities.\n",
      "\n",
      "Key Qualifications\n",
      "\n",
      "Key Qualifications\n",
      "\n",
      "Preferred Qualifications\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 5:\n",
      "What you'll bring:\n",
      "• You’re experienced with Data Science and Machine Learning (aka A.I.), including Regression, Classification, Ensemble Methods, Deep Learning, Reinforcement Learning, GenAI, etc.\n",
      "• You have strong implementation experience with at least one of the programming languages (Python, Java, C++, etc.).\n",
      "• You have strong hands-on skills in data wrangling over massive datasets using distributed computing platform.\n",
      "• 5+ years of experience mentoring junior data scientists or machine learning team.\n",
      "• You have strong written and oral communication skills.\n",
      "• You have a graduate degree in a computational science with a lot of emphasis in Machine Learning and Data Science.\n",
      "• You have experience in online advertising, recommender system, search engine, ecommerce or related areas.\n",
      "• You’re experienced with end-to-end modeling projects emerging from research efforts.\n",
      "• You have an excellent academic or industrial track record of proposing, conducting, and reporting results of original research, plus collaborative research with publications.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 6:\n",
      "o Work closely with data scientists, analysts, and other stakeholders to understand data needs and deliver solutions.\n",
      "\n",
      "o Implement CI/CD pipelines to automate deployment and testing of data infrastructure.\n",
      "\n",
      "o Stay up-to-date with the latest industry trends and technologies to continuously improve data engineering practices.\n",
      "\n",
      "Required Skills and Qualifications:\n",
      "• Technical Skills:\n",
      "\n",
      "o Hands-on SQL, Python or Scala programming.\n",
      "\n",
      "o Strong experience with SSIS, Apache Spark and Databricks.\n",
      "\n",
      "o Hands-on experience with Apache Airflow or similar workflow orchestration tools.\n",
      "\n",
      "o Knowledge of data cleansing and curation techniques.\n",
      "\n",
      "o Familiarity with Unity Catalog or other metadata management tools.\n",
      "\n",
      "o Understanding of data governance principles and best practices.\n",
      "\n",
      "o Experience with cloud platforms (AWS).\n",
      "\n",
      "o Proficiency in CI/CD tools and practices (e.g., Jenkins, GitLab CI, etc.).\n",
      "\n",
      "o Experience with JVM tuning and Spark job performance investigation.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 7:\n",
      "Minimum Qualifications\n",
      "• 5 years relevant experience with Bachelor in related field; 3 years relevant experience with Masters in related field.\n",
      "• Experience with defining requirements for using and maintaining open-source data analytics, data workflow, and database tools.\n",
      "• Develop, implement, and maintain data analytic protocols, standards, and documentation.\n",
      "• Design data models based on requirements/needs for complex analysis of data analyst\n",
      "• Define requirements for vendors to implement into their proposed solutions to meet DoD customer’s organization needs and use cases.\n",
      "• Experience working with multiple types of datasets and database technologies\n",
      "• Experience building data pipelines from end to end\n",
      "• Experience with multiple industry standard cloud and on-premise database technologies\n",
      "• Experience with data exploration, analysis, and visualization tools such as Python and Matlab\n",
      "• Experience and familiarity with Linux operating environment\n",
      "• Experience with open-source containerization technologies and tools and how to utilize those technologies as appropriately for database technologies and analytics\n",
      "• Define internal process improvements to automate repetitive tasks to minimize downtime between analysis\n",
      "• Clearance: Secret clearance to start. Must be able to obtain and maintain a TS/SCI security clearance with enhanced security checks.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 8:\n",
      "MINIMUM QUALIFICATIONS:\n",
      "• Bachelor's degree in statistics, computer science, data analytics, operations research, mathematics, physics, economics, finance, or other quantitative disciplines, and three years of related experience, OR an equivalent combination of education, training, and experience.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 9:\n",
      "o Experience with Medallion architecture for data maturity lifecycle.\n",
      "• Soft Skills:\n",
      "\n",
      "o Excellent problem-solving and analytical skills.\n",
      "\n",
      "o Strong communication and collaboration skills.\n",
      "\n",
      "o Ability to work independently and as part of a team.\n",
      "\n",
      "o Detail-oriented with a focus on delivering high-quality work.\n",
      "\n",
      "Preferred Qualifications:\n",
      "• Certification in cloud platforms (AWS Certified Data Analytics, etc.).\n",
      "• Familiarity with SQL and NoSQL databases.\n",
      "• Experience in a similar role within a fast-paced, data-driven environment.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 10:\n",
      "Option 1: Bachelor's degree in Computer Science or related field and 4 years' experience in data engineering, solution architecture, business\n",
      "intelligence, business analytics or related field. Option 2: 6 years' experience in data engineering, solution architecture, business intelligence,\n",
      "business analytics or related field. Option 3: Master's degree in Computer Science and 2 years' experience in data engineering, solution\n",
      "architecture, business intelligence, business analytics or related field.\n",
      "\n",
      "Preferred Qualifications...\n",
      "\n",
      "Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 11:\n",
      "Data Scientist- Entry Level at Idaho National Laboratory in Idaho Falls, ID. Full-time position. Salary: . Idaho National Laboratory is hiring a Data Scientist to work in our Computational Data Science department. Our team works a 9x80 schedule located out of our Idaho Falls facility with every other Friday off. You will apply mathematics, statistics, logic, and computer science to obtain insights and conclusions from large datasets. Develop and test algorithms and software-based analytical tools to operationalize those algorithms, using artificial intelligence, machine-learning, and other techniques. Adapt machine learning and artificial intelligence technologies to solve scientific problems and to extract new insights from scientific data. Responsibilities Include: - Analyze data and produce results in response to data analytics, visualization, and modeling needs on a variety of computational architectures (cloud, on-premises, edge-based). - Apply advanced technologies, such as Explainable Artificial Intelligence/Machine Learning (AI/ML), Large language models (LLMs), physics-informed machine learning process-informed Machine Learning, systems performance analysis, and data sensor fusion methodologies. - Work with a versatile team of scientists and engineers on challenging work scope. - Demonstrate effective research skills and the ability to collaborate and partner with other government agencies, academia, and private sector companies to achieve project goals. - Publish results of research and development in a mix of refereed publications and conference proceedings. - Work with other research teams to manage cross-cutting technical programs for a variety of customers. - Maintain notebooks, software, and applications. - Work with software and database developers to develop, test, and deploy software code to development, staging and production environments. - May develop software for custom solutions, including 3D visualization tools, paralleled software, batch scripting and web api interaction/development. - Analyze data quality, characterize, and communicate data quality issues to data providers, and design repeatable data processing procedures to accurately analyze data in the presence of quality issues. - Work with domain experts to understand the scope and limits of questions that can be answered by data sets, identify important data features as inputs to analysis, and design output metrics that elegantly describe system attributes and behavior. - Employ specialized methods and state-of-the-art data analysis and modeling tools to support the INL's mission by ensuring the nation's safe, and sustainable use of engineered systems in many domains by applying capabilities to impactful issues in risk, reliability, and operational performance. - Follow INL policies and lab-wide procedures related to software development, cybersecurity, and management of sensitive information. - Work with data providers, database administrators, and software developers to diagnose and with data and software systems quickly and effectively. - You will report to the Department Manager.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 12:\n",
      "Solid knowledge in statistics\n",
      "\n",
      "Descriptive and inferential statistics Univariate and multivariate analysis Discrete and continuous probability distributions Sampling and Hypothesis testing Confidence interval and statistical significance\n",
      "\n",
      "Mastery of machine learning\n",
      "\n",
      "Supervised, unsupervised, and reinforcement learning Dealing with categorical and numerical variables Dealing with missing data Feature selection and dimensionality reduction Training/test data samples Cross validation Classification algorithms and metrics Hyperparameters tuning Regression algorithms and metrics Time series analysis Anomaly detection Clustering (K-means)\n",
      "\n",
      "Required Skills Education And Experience\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 13:\n",
      "Technical Requirements & Qualifications:\n",
      "\n",
      "· Bachelor’s degree in data science, Computer Science, Information Systems, or a related field.\n",
      "\n",
      "· 3-5 years of experience as an Analyst (performed data manipulation, data analysis, data management).\n",
      "\n",
      "· Expert in Data manipulation techniques (esp. querying tables with SQL)\n",
      "\n",
      "· Experience in a commercial pharmaceuticals/biotech/healthcare company is a strong plus\n",
      "\n",
      "· Knowledge of Six Sigma and Agile practices a plus\n",
      "\n",
      "· Must be a team player with the ability to collaborate in a fast-moving environment\n",
      "\n",
      "· Preferred Knowledge of Data Warehousing, Snowflake, SQL, ETL/ELT, Informatica, AWS\n",
      "\n",
      "· Tools: JIRA, Confluence, Microsoft Suite, including Teams\n",
      "\n",
      "· Knowledge of digital data such as web, Email, social media, TV etc. highly preferred\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 14:\n",
      "What You'll Bring:\n",
      "• Active TS/SCI security clearance and the ability to work from a SCIF.\n",
      "• Based in Washington DC/Northern Virginia with occasional travel to client sites.\n",
      "• A degree in Computer Science or a related field.\n",
      "• 3+ years of hands-on experience with Python coding (pandas, numpy, sklearn, TensorFlow, PyTorch, etc.).\n",
      "• Proven experience deploying machine learning models into production environments.\n",
      "• Proficiency with SQL/NoSQL databases, version control tools (e.g., Git), Docker, and Kubernetes.\n",
      "• Strong ownership, accountability, and communication skills.\n",
      "\n",
      "Bonus Points For:\n",
      "• Experience leading projects in SCIF environments.\n",
      "• Expertise in Cyber Analytics, PCAP, or network monitoring.\n",
      "• Familiarity with Spark, Dask, Snowpark, Kafka, or task schedulers like Airflow and Celery.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 15:\n",
      "Job Competencies\n",
      "• Effective presentation, facilitation, and communication skills,\n",
      "both oral and written\n",
      "• Must be able to communicate with tact and compassion.\n",
      "• Must pay attention to detail and consistency within multiple\n",
      "documents.\n",
      "• Exceptional analytical and problem-solving skills with the\n",
      "ability to interpret complex data and present findings\n",
      "clearly.\n",
      "• Training in automatization tools, AI and cloud services\n",
      "(AWS).\n",
      "• Training in AI applications relevant to human subject\n",
      "research\n",
      "• Strong ability to manipulate and analyze large datasets,\n",
      "ensuring that the QA and reporting systems deliver accurate,\n",
      "actionable insights.\n",
      "• Must understand the service nature of the position.\n",
      "• Must be a team player.\n",
      "• Must be able to prioritize competing demands and\n",
      "deadlines.\n",
      "• Must have familiarity with federal regulations and ethical\n",
      "requirements for human subject participation in clinical, social,\n",
      "and behavioral research.\n",
      "\n",
      "Additional Information\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 16:\n",
      "Data Scientist TS/SCI at iO Associates - US in Reston, VA. Full-time position. Salary: 160K–200K a year. A leading AI Analytics startup is looking to hire a Data Scientist to work on their latest AI products. Their cutting-edge platform turns complex data into actionable insights with intuitive AI tools and advanced visualization features, empowering users to make informed decisions effortlessly. Trusted by top defense agencies and Fortune 500 companies, They are expanding rapidly and are looking for a talented Data Scientist to join their dynamic team!\n",
      "\n",
      "What You'll Do:\n",
      "• Design, implement, and deliver AI-powered solutions tailored to meet client needs using advanced analytics tools.\n",
      "• Collaborate closely with clients to guide AI solutions from concept to full-scale deployment.\n",
      "• Utilize big data tools and platforms, such as Databricks, to create scalable and efficient AI-driven solutions.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 17:\n",
      "REQUIRED\n",
      "- Bachelor's degree in a related field, or equivalent work experience\n",
      "- Three to four years of statistical and/or data analytics experience\n",
      "\n",
      "PREFERRED\n",
      "- Experience in analytics, advanced analytics/statistics, predictive modeling\n",
      "- Strong analytic skills with the ability to extract, collect, organize, analyze and interpret trends or patterns in complex data sets\n",
      "- Demonstrated project management skills\n",
      "- Effective interpersonal, verbal and written communication skills\n",
      "\n",
      "- Experience with Alteryx, Power BI, Snowflake and/or BOXI\n",
      "\n",
      "This position offers a hybrid/flexible schedule which means there's an in-office expectation of 3 or more days per week and the flexibility to work outside the office location for the other days.\n",
      "\n",
      "If there’s anything we can do to accommodate a disability during any portion of the application or hiring process, please refer to our disability accommodations for applicants.\n",
      "\n",
      "Benefits:\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 18:\n",
      "Preferred Qualifications...\n",
      "\n",
      "Outlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n",
      "\n",
      "Data science, machine learning, optimization models, PhD in Machine Learning, Computer Science, Information Technology, Operations Research, Statistics, Applied Mathematics, Econometrics, Successful completion of one or more assessments in Python, Spark, Scala, or R, Using open source frameworks (for example, scikit learn, tensorflow, torch)\n",
      "\n",
      "Primary Location...\n",
      "\n",
      "840 W California Ave, Sunnyvale, CA 94086-4828, United States of America\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 19:\n",
      "Professional experience should demonstrate strong knowledge of cloud data platform technologies, including data ingestion, storage, processing, sharing, consumption, and analytics. Furthermore, candidates need proficiency in data warehouse design, analytics technology integrations, and architecting, designing, implementing, and managing enterprise IT infrastructure on-premise and in the cloud.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 20:\n",
      "Professional experience should demonstrate strong knowledge of cloud data platform technologies, including data ingestion, storage, processing, sharing, consumption, and analytics. Furthermore, candidates need proficiency in data warehouse design, analytics technology integrations, and architecting, designing, implementing, and managing enterprise IT infrastructure on-premise and in the cloud.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "\n",
      "--- Generated Response ---\n",
      "Content only:\n",
      "To get a data scientist job, the skills and qualifications you would need based on the provided documents include:\n",
      "\n",
      "1. **Education:**\n",
      "   - Bachelor’s Degree in fields such as statistics, computer science, data analytics, operations research, mathematics, physics, economics, finance, or other quantitative disciplines.\n",
      "   - Alternatively, a Master's or PhD in related fields is preferred for advanced roles.\n",
      "\n",
      "2. **Technical Skills:**\n",
      "   - Proficiency in one or more programming languages (Python, R, SQL, Scala, Java, C/C++, etc.).\n",
      "   - Advanced analytical and statistical knowledge, including machine learning algorithms (classification, regression, clustering).\n",
      "   - Hands-on experience with data manipulation, conditioning, and big data analysis.\n",
      "   - Experience with database systems, data models, and business intelligence tools (Tableau, Power BI).\n",
      "   - Familiarity with cloud services (AWS, Azure) and version control tools (GitHub/GitLab).\n",
      "   - Knowledge of data cleansing, data governance, and metadata management tools.\n",
      "   - Understanding of continuous integration/continuous deployment (CI/CD) practices.\n",
      "\n",
      "3. **Experience:**\n",
      "   - Experience in developing and deploying machine learning models.\n",
      "   - Experience in big data ecosystems and real-time analytics platforms (Apache Spark, Hadoop).\n",
      "   - Minimum of 1-5 years of relevant work experience depending on the education background.\n",
      "\n",
      "4. **Soft Skills:**\n",
      "   - Excellent oral and written communication skills for coordination across teams.\n",
      "   - Strong problem-solving and analytical skills.\n",
      "   - Ability to collaborate effectively and independently.\n",
      "\n",
      "5. **Preferred Skills:**\n",
      "   - Experience with agile methodologies and open-source frameworks like TensorFlow, scikit-learn.\n",
      "   - Familiarity with data visualization libraries/tools.\n",
      "   - Experience with cloud data platforms and enterprise IT infrastructure on-premise and in the cloud.\n",
      "\n",
      "These skills and qualifications are broadly reflective of industry standards for someone seeking a position as a data scientist.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"RAG PORTION\"\"\"\n",
    "# Load the existing vector store with the embedding function\n",
    "db = Chroma(\n",
    "    persist_directory=persistent_directory,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"jobs_collection\"  # Match the name used in push\n",
    ")\n",
    "\n",
    "# Define the user's question\n",
    "query = \"What are the skills I need to get a data scientist job?\"\n",
    "\n",
    "# Retrieve relevant documents based on the query\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 10, \"score_threshold\": 0.25},  # Lower threshold for testing\n",
    ")\n",
    "# `k`: Top # of document hits \n",
    "# `score_threshold`: Similarity score from text document to query\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "# Display the relevant results with metadata\n",
    "print(\"\\n--- Relevant Documents ---\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "    if doc.metadata:\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")\n",
    "\n",
    "\"\"\"AGENTIC AI PORTION\"\"\"\n",
    "\n",
    "# Combine the query and the relevant document contents\n",
    "combined_input = (\n",
    "    \"Here are some documents that might help answer the question: \"\n",
    "    + query\n",
    "    + \"\\n\\nRelevant Documents:\\n\"\n",
    "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    + \"\\n\\nPlease provide an answer based only on the provided documents. If the answer is not found in the documents, respond with 'I'm not sure'.\"\n",
    ")\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Define the messages for the model\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a career advisor.\"),\n",
    "    HumanMessage(content=combined_input),\n",
    "]\n",
    "\n",
    "# Invoke the model with the combined input\n",
    "result = model.invoke(messages)\n",
    "\n",
    "# Display the full result and content only\n",
    "print(\"\\n--- Generated Response ---\")\n",
    "# print(\"Full result:\")\n",
    "# print(result)\n",
    "print(\"Content only:\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to Format Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relevant Documents ---\n",
      "Document 1:\n",
      "PREFERRED QUALIFICATIONS:\n",
      "• Master’s or PhD in in Statistics, Computer Science, Data Analytics, Operations Research, Mathematics, Physics, Economics, Finance, or other quantitative disciplines.\n",
      "• Strong problem-solving skills with an emphasis on product development.\n",
      "• Sound knowledge of statistical methods and machine learning algorithms.\n",
      "• High degree of proficiency in Python and SQL.\n",
      "• 1+ years of experience in collecting, integrating, processing, and analyzing data.\n",
      "• 1+ years of experience in using open source/commercial data visualization libraries/tools.\n",
      "• 1+ years of experience with cloud services (AWS, Azure).\n",
      "• 1+ years of experience in version control workflows and technologies (Git, GitHub/Gitlab)\n",
      "• Ability to analyze, summarize and cogently present quantitative and qualitative information.\n",
      "• Passion for implementing industry standards / best coding practices.\n",
      "• Excellent written and verbal communication skills for coordinating across teams.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 2:\n",
      "M.Sc. in Data Science or related with 1+ years' experience\n",
      "• or- B.Sc. in Data Science or related with 3+ years' experience Technical Skills Computer Science Requirements Proficient in at least one major programming language (Python, Scala, Java, C#, C/C++, Kotlin). Hands-on experience with Git (handling branching and merging) Solid knowledge of data structures, algorithm design, problem solving, and complexity analysis. Understanding of database theory and experience in at least one relational DBMS like SQL Server, PostgreSQL, and MySQL. Expertise in data manipulation\n",
      "\n",
      "Desired Skills\n",
      "\n",
      "Big Data analytics with spark Real-time analytics Apache Spark machine learning library Familiarity with frameworks and tools such as Databricks, RabbitMQ, Kafka, Docker or Kubernetes. Familiarity with Microsoft Azure Analytics Platform. Familiarity with the mining Industry, focused on load & haul operations. Experience with agile methodologies.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 3:\n",
      "MINIMUM QUALIFICATIONS:\n",
      "• Bachelor’s Degree required OR equivalent combination of training, education, and relevant experience may be considered in lieu of a degree.\n",
      "• Advanced analytical knowledge of data\n",
      "• Statistical analysis\n",
      "• Conducting big data analysis\n",
      "• Data conditioning\n",
      "• Programming advanced computing\n",
      "• Developing machine learning algorithms (classification, regression, clustering, model validation, etc.)\n",
      "• Developing software and data models\n",
      "• Executing predictive analytics\n",
      "• Proficient with one or more programming languages (Python, R, SQL, Alteryx, SAS, etc.) and hands-on working experience with database systems, business intelligence, and visual reporting tools (BOBJ, Tableau, Power BI, etc.\n",
      "\n",
      "WORK ENVIRONMENT:\n",
      "• Work is performed 4 days in the office, with 1 day remote.\n",
      "• May require traveling up to 10% of the time.\n",
      "• Additional duties as required.\n",
      "\n",
      "COMMUNICATIONS AND INTERPERSONAL SKILLS:\n",
      "\n",
      "Must have excellent oral and written communication skills.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 4:\n",
      "What you'll bring:\n",
      "• You’re experienced with Data Science and Machine Learning (aka A.I.), including Regression, Classification, Ensemble Methods, Deep Learning, Reinforcement Learning, GenAI, etc.\n",
      "• You have strong implementation experience with at least one of the programming languages (Python, Java, C++, etc.).\n",
      "• You have strong hands-on skills in data wrangling over massive datasets using distributed computing platform.\n",
      "• 5+ years of experience mentoring junior data scientists or machine learning team.\n",
      "• You have strong written and oral communication skills.\n",
      "• You have a graduate degree in a computational science with a lot of emphasis in Machine Learning and Data Science.\n",
      "• You have experience in online advertising, recommender system, search engine, ecommerce or related areas.\n",
      "• You’re experienced with end-to-end modeling projects emerging from research efforts.\n",
      "• You have an excellent academic or industrial track record of proposing, conducting, and reporting results of original research, plus collaborative research with publications.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 5:\n",
      "Minimum Qualifications\n",
      "• B.S. in a quantitative field (e.g., Statistics, Economics, Computer Science, Mathematics, Engineering).\n",
      "• At least 5+ years of experience as a Data Analytics Engineer with excellent analytical and problem-solving skills.\n",
      "• Advanced programming skills in data manipulation & processing (SQL, Python and Scala).\n",
      "• Experience in the big data ecosystem (e.g. Hadoop, Spark, Iceberg, Trino, Airflow).\n",
      "• Proven expertise in data wrangling and developing data visualizations & reporting with toolings such as Tableau, Superset etc.\n",
      "• Strong understanding of data analytics, structured and unstructured data analysis, predictive modeling techniques to implement Fraud detection, identify patterns in data and data visualization as well as a good command of emerging methodologies like artificial intelligence.\n",
      "• Outstanding verbal and written communication skills, along with strong collaborative abilities.\n",
      "\n",
      "Key Qualifications\n",
      "\n",
      "Key Qualifications\n",
      "\n",
      "Preferred Qualifications\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 6:\n",
      "o Work closely with data scientists, analysts, and other stakeholders to understand data needs and deliver solutions.\n",
      "\n",
      "o Implement CI/CD pipelines to automate deployment and testing of data infrastructure.\n",
      "\n",
      "o Stay up-to-date with the latest industry trends and technologies to continuously improve data engineering practices.\n",
      "\n",
      "Required Skills and Qualifications:\n",
      "• Technical Skills:\n",
      "\n",
      "o Hands-on SQL, Python or Scala programming.\n",
      "\n",
      "o Strong experience with SSIS, Apache Spark and Databricks.\n",
      "\n",
      "o Hands-on experience with Apache Airflow or similar workflow orchestration tools.\n",
      "\n",
      "o Knowledge of data cleansing and curation techniques.\n",
      "\n",
      "o Familiarity with Unity Catalog or other metadata management tools.\n",
      "\n",
      "o Understanding of data governance principles and best practices.\n",
      "\n",
      "o Experience with cloud platforms (AWS).\n",
      "\n",
      "o Proficiency in CI/CD tools and practices (e.g., Jenkins, GitLab CI, etc.).\n",
      "\n",
      "o Experience with JVM tuning and Spark job performance investigation.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 7:\n",
      "o Experience with Medallion architecture for data maturity lifecycle.\n",
      "• Soft Skills:\n",
      "\n",
      "o Excellent problem-solving and analytical skills.\n",
      "\n",
      "o Strong communication and collaboration skills.\n",
      "\n",
      "o Ability to work independently and as part of a team.\n",
      "\n",
      "o Detail-oriented with a focus on delivering high-quality work.\n",
      "\n",
      "Preferred Qualifications:\n",
      "• Certification in cloud platforms (AWS Certified Data Analytics, etc.).\n",
      "• Familiarity with SQL and NoSQL databases.\n",
      "• Experience in a similar role within a fast-paced, data-driven environment.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 8:\n",
      "Minimum Qualifications\n",
      "• 5 years relevant experience with Bachelor in related field; 3 years relevant experience with Masters in related field.\n",
      "• Experience with defining requirements for using and maintaining open-source data analytics, data workflow, and database tools.\n",
      "• Develop, implement, and maintain data analytic protocols, standards, and documentation.\n",
      "• Design data models based on requirements/needs for complex analysis of data analyst\n",
      "• Define requirements for vendors to implement into their proposed solutions to meet DoD customer’s organization needs and use cases.\n",
      "• Experience working with multiple types of datasets and database technologies\n",
      "• Experience building data pipelines from end to end\n",
      "• Experience with multiple industry standard cloud and on-premise database technologies\n",
      "• Experience with data exploration, analysis, and visualization tools such as Python and Matlab\n",
      "• Experience and familiarity with Linux operating environment\n",
      "• Experience with open-source containerization technologies and tools and how to utilize those technologies as appropriately for database technologies and analytics\n",
      "• Define internal process improvements to automate repetitive tasks to minimize downtime between analysis\n",
      "• Clearance: Secret clearance to start. Must be able to obtain and maintain a TS/SCI security clearance with enhanced security checks.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 9:\n",
      "Data Scientist- Entry Level at Idaho National Laboratory in Idaho Falls, ID. Full-time position. Salary: . Idaho National Laboratory is hiring a Data Scientist to work in our Computational Data Science department. Our team works a 9x80 schedule located out of our Idaho Falls facility with every other Friday off. You will apply mathematics, statistics, logic, and computer science to obtain insights and conclusions from large datasets. Develop and test algorithms and software-based analytical tools to operationalize those algorithms, using artificial intelligence, machine-learning, and other techniques. Adapt machine learning and artificial intelligence technologies to solve scientific problems and to extract new insights from scientific data. Responsibilities Include: - Analyze data and produce results in response to data analytics, visualization, and modeling needs on a variety of computational architectures (cloud, on-premises, edge-based). - Apply advanced technologies, such as Explainable Artificial Intelligence/Machine Learning (AI/ML), Large language models (LLMs), physics-informed machine learning process-informed Machine Learning, systems performance analysis, and data sensor fusion methodologies. - Work with a versatile team of scientists and engineers on challenging work scope. - Demonstrate effective research skills and the ability to collaborate and partner with other government agencies, academia, and private sector companies to achieve project goals. - Publish results of research and development in a mix of refereed publications and conference proceedings. - Work with other research teams to manage cross-cutting technical programs for a variety of customers. - Maintain notebooks, software, and applications. - Work with software and database developers to develop, test, and deploy software code to development, staging and production environments. - May develop software for custom solutions, including 3D visualization tools, paralleled software, batch scripting and web api interaction/development. - Analyze data quality, characterize, and communicate data quality issues to data providers, and design repeatable data processing procedures to accurately analyze data in the presence of quality issues. - Work with domain experts to understand the scope and limits of questions that can be answered by data sets, identify important data features as inputs to analysis, and design output metrics that elegantly describe system attributes and behavior. - Employ specialized methods and state-of-the-art data analysis and modeling tools to support the INL's mission by ensuring the nation's safe, and sustainable use of engineered systems in many domains by applying capabilities to impactful issues in risk, reliability, and operational performance. - Follow INL policies and lab-wide procedures related to software development, cybersecurity, and management of sensitive information. - Work with data providers, database administrators, and software developers to diagnose and with data and software systems quickly and effectively. - You will report to the Department Manager.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "Document 10:\n",
      "MINIMUM QUALIFICATIONS:\n",
      "• Bachelor's degree in statistics, computer science, data analytics, operations research, mathematics, physics, economics, finance, or other quantitative disciplines, and three years of related experience, OR an equivalent combination of education, training, and experience.\n",
      "\n",
      "Source: Unknown\n",
      "\n",
      "------------------CHATGPT OUTPUT------------------\n",
      "Based on the provided documents, here are the top skills needed for a data scientist position with their importance ratings:\n",
      "\n",
      "1. {{'skill': 'Proficiency in Python', 'importance': 95}}\n",
      "2. {{'skill': 'Statistical Analysis', 'importance': 90}}\n",
      "3. {{'skill': 'Machine Learning Algorithms', 'importance': 90}}\n",
      "4. {{'skill': 'Data Wrangling and Manipulation', 'importance': 88}}\n",
      "5. {{'skill': 'SQL Programming', 'importance': 85}}\n",
      "6. {{'skill': 'Cloud Services (AWS, Azure)', 'importance': 83}}\n",
      "7. {{'skill': 'Data Visualization Tools (e.g. Tableau, Power BI)', 'importance': 82}}\n",
      "8. {{'skill': 'Experience with Big Data Tools (Hadoop, Spark)', 'importance': 80}}\n",
      "9. {{'skill': 'Excellent Written and Verbal Communication', 'importance': 78}}\n",
      "10. {{'skill': 'Experience with Version Control (Git, GitLab)', 'importance': 75}}\n",
      "11. {{'skill': 'Hands-on Experience with Data Analysis and Modeling', 'importance': 75}}\n",
      "12. {{'skill': 'Programming Skills in R or Scala', 'importance': 70}}\n",
      "13. {{'skill': 'Understanding of Database Systems', 'importance': 70}}\n",
      "14. {{'skill': 'Ability to Develop Predictive Models', 'importance': 68}}\n",
      "15. {{'skill': 'Experience with Container Tools (Docker, Kubernetes)', 'importance': 65}}\n",
      "\n",
      "These skills have been ranked based on their frequency and emphasis in the job requirements provided in the documents.\n",
      "------------------CHATGPT OUTPUT------------------\n",
      "\n",
      "--- Generated Response ---\n",
      "Content only:\n",
      "Based on the provided documents, here are the top skills needed for a data scientist position with their importance ratings:\n",
      "\n",
      "1. {{'skill': 'Proficiency in Python', 'importance': 95}}\n",
      "2. {{'skill': 'Statistical Analysis', 'importance': 90}}\n",
      "3. {{'skill': 'Machine Learning Algorithms', 'importance': 90}}\n",
      "4. {{'skill': 'Data Wrangling and Manipulation', 'importance': 88}}\n",
      "5. {{'skill': 'SQL Programming', 'importance': 85}}\n",
      "6. {{'skill': 'Cloud Services (AWS, Azure)', 'importance': 83}}\n",
      "7. {{'skill': 'Data Visualization Tools (e.g. Tableau, Power BI)', 'importance': 82}}\n",
      "8. {{'skill': 'Experience with Big Data Tools (Hadoop, Spark)', 'importance': 80}}\n",
      "9. {{'skill': 'Excellent Written and Verbal Communication', 'importance': 78}}\n",
      "10. {{'skill': 'Experience with Version Control (Git, GitLab)', 'importance': 75}}\n",
      "11. {{'skill': 'Hands-on Experience with Data Analysis and Modeling', 'importance': 75}}\n",
      "12. {{'skill': 'Programming Skills in R or Scala', 'importance': 70}}\n",
      "13. {{'skill': 'Understanding of Database Systems', 'importance': 70}}\n",
      "14. {{'skill': 'Ability to Develop Predictive Models', 'importance': 68}}\n",
      "15. {{'skill': 'Experience with Container Tools (Docker, Kubernetes)', 'importance': 65}}\n",
      "\n",
      "These skills have been ranked based on their frequency and emphasis in the job requirements provided in the documents.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "\n",
    "#  Pydantic model to structure the skills and their importance\n",
    "class SkillData(BaseModel):\n",
    "    skill: str\n",
    "    importance: int\n",
    "\n",
    "class SkillsResponse(BaseModel):\n",
    "    skills: List[SkillData]\n",
    "\n",
    "\"\"\"RAG PORTION\"\"\"\n",
    "# Load the existing vector store with the embedding function\n",
    "db = Chroma(\n",
    "    persist_directory=persistent_directory,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"jobs_collection\"  # Match the name used in push\n",
    ")\n",
    "\n",
    "# Define the user's question\n",
    "query = \"What are the top skills I need to get a data scientist job?\"\n",
    "\n",
    "# Retrieve relevant documents based on the query\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 10, \"score_threshold\": 0.25},  # Lower threshold for testing\n",
    ")\n",
    "# `k`: Top # of document hits \n",
    "# `score_threshold`: Similarity score from text document to query\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "# Display the relevant results with metadata\n",
    "print(\"\\n--- Relevant Documents ---\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "    if doc.metadata:\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\") # DEBUG\n",
    "\n",
    "\"\"\"AGENTIC AI PORTION\"\"\"\n",
    "\n",
    "# Combine the query and the relevant document contents\n",
    "combined_input = (\n",
    "    \"Here are some documents that might help answer the question: \"\n",
    "    + query\n",
    "    + \"\\n\\nRelevant Documents:\\n\"\n",
    "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    + \"\\n\\nPlease extract the top skills for the following job posting and their importance (on a scale from 1 to 100):\\nReturn the skills in this format: [{{'skill': 'Skill Name', 'importance': 95}}, ...]\"\n",
    ")\n",
    "\n",
    "# Define the messages for the model\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a career advisor.\"),\n",
    "    HumanMessage(content=combined_input),\n",
    "]\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Invoke the model with the combined input\n",
    "result = model.invoke(messages)\n",
    "print(\"------------------CHATGPT OUTPUT------------------\")\n",
    "print(result.content)\n",
    "print(\"------------------CHATGPT OUTPUT------------------\")\n",
    "\n",
    "# NOTE: need to extract the skills set \n",
    "\n",
    "\n",
    "\"\"\" OUTPUT \"\"\"\n",
    "# plot_skills(structured_skills)\n",
    "\n",
    "# Display the full result and content only\n",
    "print(\"\\n--- Generated Response ---\")\n",
    "# print(\"Full result:\")\n",
    "# print(result)\n",
    "print(\"Content only:\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Here's a list of the top skills extracted from the job posting along with their importance:\\n\\n1. `{{'skill': 'Machine Learning and Artificial Intelligence', 'importance': 95}}`\\n2. `{{'skill': 'Experience with programming languages (Python, Java, C++)', 'importance': 90}}`\\n3. `{{'skill': 'Data Wrangling over massive datasets using distributed computing platform', 'importance': 88}}`\\n4. `{{'skill': 'Strong analytical and problem-solving skills', 'importance': 85}}`\\n5. `{{'skill': 'Statistical Analysis', 'importance': 83}}`\\n6. `{{'skill': 'Data Visualization and reporting tools (e.g., Tableau)', 'importance': 82}}`\\n7. `{{'skill': 'Understanding of big data ecosystem (Hadoop, Spark)', 'importance': 80}}`\\n8. `{{'skill': 'Advanced knowledge in SQL', 'importance': 78}}`\\n9. `{{'skill': 'Experience with cloud platforms (AWS, Azure)', 'importance': 77}}`\\n10. `{{'skill': 'Excellent written and verbal communication skills', 'importance': 76}}`\\n11. `{{'skill': 'Familiarity with database technologies and data modeling', 'importance': 75}}`\\n12. `{{'skill': 'Ability to execute predictive analytics and develop data models', 'importance': 74}}`\\n13. `{{'skill': 'Experience with emerging methodologies (GenAI, Deep Learning)', 'importance': 72}}`\\n14. `{{'skill': 'Experience in online advertising, recommender systems, or related areas', 'importance': 70}}`\\n15. `{{'skill': 'Experience with version control systems (Git, GitHub)', 'importance': 70}}`\\n16. `{{'skill': 'Collaboration and teamwork skills', 'importance': 68}}`\\n17. `{{'skill': 'Familiarity with CI/CD pipelines and data infrastructure automation', 'importance': 66}}`\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 430, 'prompt_tokens': 2100, 'total_tokens': 2530, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6ec83003ad', 'finish_reason': 'stop', 'logprobs': None} id='run-80b47ed9-3520-4db5-87f7-044026b2ad50-0' usage_metadata={'input_tokens': 2100, 'output_tokens': 430, 'total_tokens': 2530, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "\n",
    "#  Pydantic model to structure the skills and their importance\n",
    "class SkillData(BaseModel):\n",
    "    skill: str\n",
    "    importance: int\n",
    "\n",
    "class SkillsResponse(BaseModel):\n",
    "    skills: List[SkillData]\n",
    "\n",
    "\"\"\"RAG PORTION\"\"\"\n",
    "# Load the existing vector store with the embedding function\n",
    "db = Chroma(\n",
    "    persist_directory=persistent_directory,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"jobs_collection\"  # Match the name used in push\n",
    ")\n",
    "\n",
    "# Define the user's question\n",
    "query = \"What are the top skills I need to get a data scientist job?\"\n",
    "\n",
    "# Retrieve relevant documents based on the query\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 10, \"score_threshold\": 0.25},  # Lower threshold for testing\n",
    ")\n",
    "\n",
    "# Combine the query and the relevant document contents\n",
    "combined_input = (\n",
    "    \"Here are some documents that might help answer the question: \"\n",
    "    + query\n",
    "    + \"\\n\\nRelevant Documents:\\n\"\n",
    "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    + \"\\n\\nPlease extract the top skills for the following job posting and their importance (on a scale from 1 to 100):\\nReturn the skills in this format: [{{'skill': 'Skill Name', 'importance': 95}}, ...]\"\n",
    ")\n",
    "\n",
    "# Use the LLM (ChatGPT) to extract the skills and importance scores\n",
    "response = ChatOpenAI(model=\"gpt-4o\").invoke(combined_input)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a list of the top skills extracted from the job posting along with their importance:\n",
      "\n",
      "1. `{{'skill': 'Machine Learning and Artificial Intelligence', 'importance': 95}}`\n",
      "2. `{{'skill': 'Experience with programming languages (Python, Java, C++)', 'importance': 90}}`\n",
      "3. `{{'skill': 'Data Wrangling over massive datasets using distributed computing platform', 'importance': 88}}`\n",
      "4. `{{'skill': 'Strong analytical and problem-solving skills', 'importance': 85}}`\n",
      "5. `{{'skill': 'Statistical Analysis', 'importance': 83}}`\n",
      "6. `{{'skill': 'Data Visualization and reporting tools (e.g., Tableau)', 'importance': 82}}`\n",
      "7. `{{'skill': 'Understanding of big data ecosystem (Hadoop, Spark)', 'importance': 80}}`\n",
      "8. `{{'skill': 'Advanced knowledge in SQL', 'importance': 78}}`\n",
      "9. `{{'skill': 'Experience with cloud platforms (AWS, Azure)', 'importance': 77}}`\n",
      "10. `{{'skill': 'Excellent written and verbal communication skills', 'importance': 76}}`\n",
      "11. `{{'skill': 'Familiarity with database technologies and data modeling', 'importance': 75}}`\n",
      "12. `{{'skill': 'Ability to execute predictive analytics and develop data models', 'importance': 74}}`\n",
      "13. `{{'skill': 'Experience with emerging methodologies (GenAI, Deep Learning)', 'importance': 72}}`\n",
      "14. `{{'skill': 'Experience in online advertising, recommender systems, or related areas', 'importance': 70}}`\n",
      "15. `{{'skill': 'Experience with version control systems (Git, GitHub)', 'importance': 70}}`\n",
      "16. `{{'skill': 'Collaboration and teamwork skills', 'importance': 68}}`\n",
      "17. `{{'skill': 'Familiarity with CI/CD pipelines and data infrastructure automation', 'importance': 66}}`\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documents and qualifications, here are the top skills extracted for a data scientist position, along with their importance:\n",
      "\n",
      "1. {{'skill': 'Programming Proficiency (Python, SQL)', 'importance': 95}}\n",
      "2. {{'skill': 'Machine Learning and AI Techniques', 'importance': 90}}\n",
      "3. {{'skill': 'Data Wrangling and Data Analysis', 'importance': 88}}\n",
      "4. {{'skill': 'Statistical Methods and Analysis', 'importance': 85}}\n",
      "5. {{'skill': 'Data Visualization and Reporting Tools (Tableau, Power BI)', 'importance': 80}}\n",
      "6. {{'skill': 'Cloud Services Experience (AWS, Azure)', 'importance': 78}}\n",
      "7. {{'skill': 'Big Data Technologies (Hadoop, Spark)', 'importance': 75}}\n",
      "8. {{'skill': 'Strong Problem-Solving Skills', 'importance': 73}}\n",
      "9. {{'skill': 'Version Control (Git, GitHub/GitLab)', 'importance': 70}}\n",
      "10. {{'skill': 'Excellent Communication Skills', 'importance': 68}}\n",
      "11. {{'skill': 'Collaboration and Teamwork', 'importance': 65}}\n",
      "12. {{'skill': 'Model Deployment and Maintenance', 'importance': 60}}\n",
      "\n",
      "These skills are ranked based on their frequency and emphasis across the provided job postings and qualifications documents, indicating their critical importance for excelling in a data scientist role.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to format the returned `result.content` into a nice dict of the format that is expected by `SkillsResponse.parse_obj()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| skills_matched: [('Programming Proficiency (Python, SQL)', '95'),\n",
      "                     ('Machine Learning and AI Techniques', '90'),\n",
      "                     ('Data Wrangling and Data Analysis', '88'),\n",
      "                     ('Statistical Methods and Analysis', '85'),\n",
      "                     ('Data Visualization and Reporting Tools (Tableau, Power BI)', '80'),\n",
      "                     ('Cloud Services Experience (AWS, Azure)', '78'),\n",
      "                     ('Big Data Technologies (Hadoop, Spark)', '75'),\n",
      "                     ('Strong Problem-Solving Skills', '73'),\n",
      "                     ('Version Control (Git, GitHub/GitLab)', '70'),\n",
      "                     ('Excellent Communication Skills', '68'),\n",
      "                     ('Collaboration and Teamwork', '65'),\n",
      "                     ('Model Deployment and Maintenance', '60')]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, tuple found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m skills_matched \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(pattern, result\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m      9\u001b[0m ic(skills_matched)\n\u001b[1;32m---> 10\u001b[0m skills_matched \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskills_matched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m skills_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(skills_matched)\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, tuple found"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from icecream import ic\n",
    "# Get the matches of {'skill': <>, 'importance': <>} using regular expression\n",
    "\n",
    "pattern = r\"\\{\\{'skill': '(.*?)', 'importance': (\\d{2})\\}\\}\"\n",
    "\n",
    "\n",
    "skills_matched = re.findall(pattern, result.content)\n",
    "ic(skills_matched)\n",
    "\n",
    "data = {}\n",
    "for skill, rank in skills_matched:\n",
    "    data[skill] = rank\n",
    "\n",
    "# Then graph out this dict\n",
    "\n",
    "def plot_skills(skills_response: SkillsResponse):\n",
    "    skills = [skill.skill for skill in skills_response.skills]\n",
    "    importance = [skill.importance for skill in skills_response.skills]\n",
    "    \n",
    "    # Create the plot\n",
    "    skills_df = pd.DataFrame({\n",
    "        \"Skill\": skills,\n",
    "        \"Importance\": importance\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(skills_df[\"Skill\"], skills_df[\"Importance\"], color=\"skyblue\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.title(\"Top Skills for Job Role\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
